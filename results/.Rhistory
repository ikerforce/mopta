# Check relationship
TSS; RSS+BSS # Sw
# F statistic
TSS-BSS
F = (BSS/(I-1))/(RSS/(n-I));F
## The associated p-value is:
pf(F,I-1,n-I,lower.tail=FALSE)
## For inference, we implicitly made the assumption that the residuals were
## normally distributed and that the error variance was constant.
## To validate these assumptions, the following plots and tests can be made:
# Normality
qqnorm(iris$Sepal.Width)
qqnorm(Y-X%*%beta) # qqnorm plot seems ok
anova(lm(Sepal.Width~Species,data=iris)) #anova(aov(Sepal.Width~Species,data=iris))
mod=anova(lm(Sepal.Width~Species,data=iris)) #anova(aov(Sepal.Width~Species,data=iris))
coef(mod)
mod
mod <- m(Sepal.Width~Species,data=iris)
mod <- lm(Sepal.Width~Species,data=iris)
coef(mod)
Y <- iris$Sepal.Width
mu <- rep(1, nrow(iris))
X <- model.matrix(~Species - 1, data = iris)
I = ncol(X)
X <- cbind(mu , X)
n = nrow(X)
beta <- aggregate(Y, list(iris$Species), mean )[2]
beta <- c(0, beta$x)
beta
# Compute the least squares estimates
Y=iris$Petal.Width
X=model.matrix(Sepal.Width~Species-1,data=iris);X
I = ncol(X);I
beta=solve(t(X)%*%X) %*% t(X)%*%Y; beta # the same as level averages ** solve=inverse in R
X=model.matrix(Petal.Width~Species-1,data=iris);X
I = ncol(X);I
beta=solve(t(X)%*%X) %*% t(X)%*%Y; beta # the same as level averages ** solve=inverse in R
## Quantities of interest are then:
# Total sum of squares
TSS <- sum((Y-mean(Y))^2);TSS
# Residual sum of squares
RSS <- sum((Y-X%*%beta)^2);RSS # S sigma
# Between sum of squares
BSS <- sum((X%*%beta-mean(Y))^2); BSS
# Check relationship
TSS; RSS+BSS # Sw
# F statistic
TSS-BSS
F = (BSS/(I-1))/(RSS/(n-I));F
## a)
n = nrow(iris); n
## Under the standard parametrization  mu=0,
## least squares estimates of unknown parameters
## are the corresponding averages of factor levels:
tapply(iris$Sepal.Width,iris$Species,mean)
# Compute the least squares estimates
Y=iris$Sepal.Width
X=model.matrix(Sepal.Width~Species-1,data=iris);X
I = ncol(X);I
beta=solve(t(X)%*%X) %*% t(X)%*%Y; beta # the same as level averages ** solve=inverse in R
## Quantities of interest are then:
# Total sum of squares
TSS <- sum((Y-mean(Y))^2);TSS
# Residual sum of squares
RSS <- sum((Y-X%*%beta)^2);RSS # S sigma
# Between sum of squares
BSS <- sum((X%*%beta-mean(Y))^2); BSS
# Check relationship
TSS; RSS+BSS # Sw
# F statistic
TSS-BSS
F = (BSS/(I-1))/(RSS/(n-I));F
## b)
## An ANOVA table is composed of the following quantities:
n=nrow(iris) # number of observations
I=ncol(X) # number of levels in the factor
MSS=BSS/(I-1); MSS # Mean sum of squares
MRSS=RSS/(n-I); MRSS # Mean residual sum of squares
# F-statistic
F=MSS/MRSS; F
## The associated p-value is:
pf(F,I-1,n-I,lower.tail=FALSE)
## For inference, we implicitly made the assumption that the residuals were
## normally distributed and that the error variance was constant.
## To validate these assumptions, the following plots and tests can be made:
# Normality
qqnorm(Y-X%*%beta) # qqnorm plot seems ok
shapiro.test(Y-X%*%beta) # normality seems ok
plot(X%*%beta,Y-X%*%beta) # Fitted vs residuals
bartlett.test(Sepal.Width~Species-1,data=iris) # H0: homoscedasticity
MSS=BSS/(I-1); MSS # Mean sum of squares
MRSS=RSS/(n-I); MRSS # Mean residual sum of squares
i r i s
mod=anova(lm(Sepal.Width~Species,data=iris)) #anova(aov(Sepal.Width~Species,data=iris))
# P-values: anova(lm(Sepal.Width~Species,data=iris))[1,5]
anova(lm(Sepal.Width~Species,data=iris))[1,5]
anova(lm(Sepal.Width~Species,data=iris))
X = model.matrix(Sepal.Width ~ Species + 0, iris)
X
top<-4-2
bottom = sqrt(1.1)
top/bottom
qt(0.95,97)
qt(0.975,97)
qt(0.975,97)*sqrt(1.1)
4-qt(0.975,97)*sqrt(1.1)
4+qt(0.975,97)*sqrt(1.1)
qt(0.975, 96)
qt(0.99, 96)
## Choose the true parameters and simulation setting
theta=c(0.8,10,1.5); n=100
val=c(0,0.5,1,1.5,2,2.5,3,3.5,4)
x=sample(val,n,replace=TRUE) # sample "observed" x-values # or x=runif(n,0,4)
## a)
## nonlinear regression function
f=function(x,theta)return(theta[1]*x+theta[2]/(theta[3]+3*x^2))
y=f(x,theta)
# True curve
xtrue=seq(from=0,to=4,length=100); ytrue=f(xtrue,theta)
plot(xtrue,ytrue,type="l",lwd=2,col="red")
# Generating data with the true sigma=0.5
y=f(x,theta)+rnorm(n,mean=0,sd=0.5)
plot(x,y) # scatter plot of simulated data
lines(xtrue,ytrue,lwd=2,col="red") # along with the true curve
# Fitting nonlinear model
model<-nls(y~theta1*x+theta2/(theta3+3*x^2),data=
data.frame(y=y,x=x),start=list(theta1=0.4,theta2=5,theta3=1))
# Quantities of interest
hat.th=coef(model); hat.th # estimates for theta
summary(model) # info about estimates and other staff
(summary(model)$sigma)^2 # the estimator for sigma^2
vcov(model) # the estimated covariance matrix for hat.th
# Adding the fitted curve (blue line)
lines(xtrue,f(xtrue,coef(model)),col="blue") # very good fit
## b)
# Determine 96% confidence intervals for the parameters
lb=coef(model)-qt(0.98,n-length(hat.th))*sqrt(diag(vcov(model)))
ub=coef(model)+qt(0.98,n-length(hat.th))*sqrt(diag(vcov(model)))
ci=cbind(lb,ub);rownames(ci)=names(coef(model));ci
# Bootstrap procedure
B=1000
par.boot=matrix(NA,B,length(coef(model)))
resp.boot=matrix(NA,B,length(xtrue))
rownames(par.boot)=rownames(resp.boot)<-paste("B",1:B,sep="")
colnames(par.boot)=names(coef(model))
myres=resid(model)-mean(resid(model))
resid(model)-mean(resid(model))
for(b in 1:B){
#cat("b = ",b,"\n")
# Bootstrap samples from (centered) residuals
res=sample(myres,replace=T)
# Calculate bootstrap values for the response
yboot=fitted(model)+res
# Fit model using new response and get bootstrap estimates
# for parameter and the mean values
modelBoot=nls(y~theta1*x+theta2/(theta3+3*x^2),data=
data.frame(y=yboot,x=x),start=list(theta1=0.6,theta2=7,theta3=1))
# Store estimated (by bootstrap) parameters and predictions
par.boot[b,]=coef(modelBoot) #\theta*_1,..., \theta*_B
resp.boot[b,]=predict(modelBoot,newdata=data.frame(x=xtrue))
}
# Bootstrap 96% confidence intervals for thetas
alpha=0.04
ci.boot=cbind(2*coef(model)-apply(par.boot,2,quantile,prob=1-alpha/2),
2*coef(model)-apply(par.boot,2,quantile,prob=alpha/2))
colnames(ci.boot)=colnames(ci) #or colnames(confint(model))
coef(model)
ci; ci.boot # bootstrap intervals appear to be smaller
## c)
# 98% confidence interval for f(3,theta)
# first the function that computes the gradient at x
gradvec=function(x,theta)
return(c(x,1/(theta[3]+3*x^2),-theta[2]/(theta[3]+3*x^2)^2))
v=gradvec(3,coef(model)) # compute the gradient at x=3,theta=coef(model)
# compute the estimate of f(3,theta)=2.750877
est=f(3,coef(model));est # or: predict(model,newdata=data.frame(x=3))
marg=qt(0.99,n-length(coef(model)))*sqrt(v%*%vcov(model)%*%v)
c(est-marg,est+marg)
# 98% bootstrap confidence interval for f(3,theta)
# as xtrue[75]=3, resp.boot[,75] contains the draws f(3,theta*_b)
alpha=0.02
lb.boot=2*f(3,coef(model))-quantile(resp.boot[,75],prob=1-alpha/2)
ub.boot=2*f(3,coef(model))-quantile(resp.boot[,75],prob=alpha/2)
c(lb.boot,ub.boot)
## d)
### Construct the 98%-CI for f(x,theta), for all x from [0,4]
# gradient function is required
grad=function(xx,theta){cbind(xx,1/(theta[3]+3*xx^2),
(-theta[2])/(theta[3]+3*xx^2)^2)}
mygrad<-grad(xx=x,theta=coef(model))
vals<-sqrt(apply(mygrad,1,function(xx) t(xx)%*% vcov(model)%*%xx))
# estimated mean response values
ypred=f(xtrue,coef(model)) #or:ypred=predict(model,newdata=data.frame(x=xtrue))
# 98% confidence intervals for the predicted mean response values
lb<-ypred-qt(0.99,n-length(coef(model)))*vals
ub<-ypred+qt(0.99,n-length(coef(model)))*vals
# Display classical 98% confidence bands for the mean response
plot(x,y)
polygon(c(xtrue,rev(xtrue)),c(lb,rev(ub)),col="grey",border=NA)
lines(xtrue,ytrue,col="red") # the true curve
lines(xtrue,ypred,col="blue") # its estimate
## Another graph, by using segment command
plot(xtrue,ypred,t="l",col="blue") # estimate curve
# 98% confidence intervals for the predicted mean response values
segments(xtrue,lb,xtrue,ub,col="black")
lines(xtrue,ytrue,col="red") # true curve
## e)
# The hypothesis H0: th1=th2 is the same as H0:th1-th2=0 which is obtained as
#  beta=t(a)%*%theta=0 with a=c(1,-1,0), so the test is in same way as for f(x,theta)
a=c(1,-1,0); hat.beta=a%*%hat.th # so the test is
abs(hat.beta)/sqrt(a%*%vcov(model)%*%a)>qt(0.975,97) # yes, so reject H0.
library(MASS) # or utils::data(stormer,package="MASS")
attach(stormer)
n=length(Time) #23
## a)
# first initial value for theta by using the suggestion by Williams (1959)
storm.mod0=lm(Wt*Time~Viscosity+Time-1,data=stormer)
th.ini=coef(storm.mod0); names(th.ini)=c("th1","th2"); th.ini
#take th.ini to be initial value in nls
storm.mod1=nls(Time~th1*Viscosity/(Wt-th2),data=stormer,start=th.ini)
hat.th=coef(storm.mod1); hat.th ## the LSE estimates of th1 and th2
# residual sum of squares RSS=sum(resid(storm.mod1)^2)=825.1, the same as
RSS=deviance(storm.mod1)
# the estimate of sigma^2 is var.hat=RSS/(n-p)=RSS/(n-2)
var.hat=RSS/(n-2)  # can also be extracted from summary: 6.268^2=39.28782
## b)
summary(storm.mod1) # compute the test statistics to test H0: th2=2
abs(coef(storm.mod1)[2]-2)/0.6655>qt(0.975,n-2) # No, so H0 is not rejected
# the estimated covariance matrix for hat.th is
cov.est=vcov(storm.mod1);cov.est
# now compute directly the two confidence intervals
lb=numeric(2); ub=numeric(2) #rownames(lb)=names(coef(storm.mod1))
lb=hat.th-qt(0.975,n-length(hat.th))*sqrt(diag(cov.est))
ub=hat.th+qt(0.975,n-length(hat.th))*sqrt(diag(cov.est))
ci=cbind(lb,ub);rownames(ci)=names(hat.th);ci
# if we use the normal quantiles, we obtain slightly smaller CI's
for(i in 1:2) {lb[i]=hat.th[i]-qnorm(0.975)*sqrt(cov.est[i,i])
ub[i]=hat.th[i]+qnorm(0.975)*sqrt(cov.est[i,i])}
ci=cbind(lb,ub);rownames(ci)=names(hat.th);ci
## d)
## 95%-confidence interval for the mean response f(100,60,theta)
f=function(x,theta)return(theta[1]*x[1]/(x[2]-theta[2]))
## estimate of the mean response f(100,60,theta)
f.res=f(c(100,60),hat.th); f.res
# compute the gradient vector (represeted as column)
grad<-function(x,theta){c(theta[1]/(x[2]-theta[1]),
theta[1]*x[1]/(x[2]-theta[1])^2)}
gradvec=grad(c(100,60),hat.th);names(gradvec)=c("grad1","grad2")
se=sqrt(t(gradvec)%*%vcov(storm.mod1)%*%gradvec) # standard error
lb=f.res-qt(0.975,n-length(hat.th))*se
ub=f.res+qt(0.975,n-length(hat.th))*se
c(lb,ub) # approximate 95%-confidence interval for f(100,60,theta)
## e)
th1=th.ini[1]
storm.mod2=nls(Time~th1*Viscosity/Wt,data=stormer,start=th1)
# smaller model omega with theta2=0
anova(storm.mod1,storm.mod2) # smaller model omega is not good
AIC(storm.mod1);AIC(storm.mod2) # also AIC is beter for the full model
### Finally check the model assumptions # the students have to do this
# residuals against the fitted values
plot(fitted(storm.mod1),resid(storm.mod1)) # not really ok
qqnorm(resid(storm.mod1)) # ok
hist(resid(storm.mod1)) # ok
## Choose the true parameters and simulation setting
theta=c(0.8,10,1.5); n=100
val=c(0,0.5,1,1.5,2,2.5,3,3.5,4)
x=sample(val,n,replace=TRUE) # sample "observed" x-values # or x=runif(n,0,4)
## a)
## nonlinear regression function
f=function(x,theta)return(theta[1]*x+theta[2]/(theta[3]+3*x^2))
y=f(x,theta)
# True curve
xtrue=seq(from=0,to=4,length=100); ytrue=f(xtrue,theta)
plot(xtrue,ytrue,type="l",lwd=2,col="red")
# Generating data with the true sigma=0.5
y=f(x,theta)+rnorm(n,mean=0,sd=0.5)
plot(x,y) # scatter plot of simulated data
lines(xtrue,ytrue,lwd=2,col="red") # along with the true curve
## Choose the true parameters and simulation setting
theta=c(0.8,10,1.5); n=100
x=runif(100,0,4)
## a)
## nonlinear regression function
f=function(x,theta)return(theta[1]*x+theta[2]/(theta[3]+3*x^2))
y=f(x,theta)
# True curve
xtrue=seq(from=0,to=4,length=100); ytrue=f(xtrue,theta)
plot(xtrue,ytrue,type="l",lwd=2,col="red")
# Generating data with the true sigma=0.5
y=f(x,theta)+rnorm(n,mean=0,sd=0.5)
plot(x,y) # scatter plot of simulated data
lines(xtrue,ytrue,lwd=2,col="red") # along with the true curve
# Fitting nonlinear model
model<-nls(y~theta1*x+theta2/(theta3+3*x^2),data=
data.frame(y=y,x=x),start=list(theta1=0.4,theta2=5,theta3=1))
# Quantities of interest
hat.th=coef(model); hat.th # estimates for theta
summary(model) # info about estimates and other staff
(summary(model)$sigma)^2 # the estimator for sigma^2
vcov(model) # the estimated covariance matrix for hat.th
# Adding the fitted curve (blue line)
lines(xtrue,f(xtrue,coef(model)),col="blue") # very good fit
## b)
# Determine 96% confidence intervals for the parameters
lb=coef(model)-qt(0.98,n-length(hat.th))*sqrt(diag(vcov(model)))
ub=coef(model)+qt(0.98,n-length(hat.th))*sqrt(diag(vcov(model)))
ci=cbind(lb,ub);rownames(ci)=names(coef(model));ci
# Bootstrap procedure
B=1000
par.boot=matrix(NA,B,length(coef(model)))
resp.boot=matrix(NA,B,length(xtrue))
rownames(par.boot)=rownames(resp.boot)<-paste("B",1:B,sep="")
colnames(par.boot)=names(coef(model))
myres=resid(model)-mean(resid(model)) # center the residuals!
for(b in 1:B){
#cat("b = ",b,"\n")
# Bootstrap samples from (centered) residuals
res=sample(myres,replace=T)
# Calculate bootstrap values for the response
yboot=fitted(model)+res
# Fit model using new response and get bootstrap estimates
# for parameter and the mean values
modelBoot=nls(y~theta1*x+theta2/(theta3+3*x^2),data=
data.frame(y=yboot,x=x),start=list(theta1=0.6,theta2=7,theta3=1))
# Store estimated (by bootstrap) parameters and predictions
par.boot[b,]=coef(modelBoot) #\theta*_1,..., \theta*_B
resp.boot[b,]=predict(modelBoot,newdata=data.frame(x=xtrue))
}
# Bootstrap 96% confidence intervals for thetas
alpha=0.04
ci.boot=cbind(2*coef(model)-apply(par.boot,2,quantile,prob=1-alpha/2),
2*coef(model)-apply(par.boot,2,quantile,prob=alpha/2))
colnames(ci.boot)=colnames(ci) #or colnames(confint(model))
coef(model)
ci; ci.boot # bootstrap intervals appear to be smaller
## c)
# 98% confidence interval for f(3,theta)
# first the function that computes the gradient at x
gradvec=function(x,theta)
return(c(x,1/(theta[3]+3*x^2),-theta[2]/(theta[3]+3*x^2)^2))
v=gradvec(3,coef(model)) # compute the gradient at x=3,theta=coef(model)
# compute the estimate of f(3,theta)=2.750877
est=f(3,coef(model));est # or: predict(model,newdata=data.frame(x=3))
marg=qt(0.99,n-length(coef(model)))*sqrt(v%*%vcov(model)%*%v)
c(est-marg,est+marg)
# 98% bootstrap confidence interval for f(3,theta)
# as xtrue[75]=3, resp.boot[,75] contains the draws f(3,theta*_b)
alpha=0.02
lb.boot=2*f(3,coef(model))-quantile(resp.boot[,75],prob=1-alpha/2)
ub.boot=2*f(3,coef(model))-quantile(resp.boot[,75],prob=alpha/2)
c(lb.boot,ub.boot)
## d)
### Construct the 98%-CI for f(x,theta), for all x from [0,4]
# gradient function is required
grad=function(xx,theta){cbind(xx,1/(theta[3]+3*xx^2),
(-theta[2])/(theta[3]+3*xx^2)^2)}
mygrad<-grad(xx=x,theta=coef(model))
vals<-sqrt(apply(mygrad,1,function(xx) t(xx)%*% vcov(model)%*%xx))
# estimated mean response values
ypred=f(xtrue,coef(model)) #or:ypred=predict(model,newdata=data.frame(x=xtrue))
# 98% confidence intervals for the predicted mean response values
lb<-ypred-qt(0.99,n-length(coef(model)))*vals
ub<-ypred+qt(0.99,n-length(coef(model)))*vals
# Display classical 98% confidence bands for the mean response
plot(x,y)
polygon(c(xtrue,rev(xtrue)),c(lb,rev(ub)),col="grey",border=NA)
for (i in 1:100) i+1
x = i
for (i in 1:100) i+1
x = i
x = c(1,2,2,3)
y = c(1,2,2,4)
pmax(x,y)
x = c(1,2,2,3)
y = c(1,2,2,4)
pmax(x,y)
V = c(0,0)
P1=matrix(c(0.5,0.5,0.1), nrow=2, byrow=T)
V = c(0,0)
P1=matrix(c(0.5,0.5,0,1), nrow=2, byrow=T)
View(P1)
P2=matrix(c(0.5,4), nrow=2)
View(P2)
for(n in 1:100){
V = pmax(c(3,1) + P1 %*% V, c(3,0) + P2 %&% V)
}
for(n in 1:100){
V = pmax(c(3,1) + P1 %*% V, c(3,0) + P2 %*% V)
}
for(n in 1:100){
V = pmax(c(3,1) + P1%*%V, c(3,0) + P2%*%V)
}
V = c(0,0)
P1=matrix(c(0.5,0.5,0,1), nrow=2, byrow=T)
P2=matrix(c(0.5,4), nrow=2)
for(n in 1:100){
V = pmax(c(3,1)+P1%*%V, c(3,0)+P2%*%V)
}
P2=matrix(rep(0.5,4), nrow=2)
for(n in 1:100){
V = pmax(c(3,1)+P1%*%V, c(3,0)+P2%*%V)
}
V = c(0,0)
P1=matrix(c(0.5,0.5,0,1), nrow=2, byrow=T)
P1%*%V
c(3,1)+P1%*%V
c(3,0)+P2%*%V
V = c(3,1)+P1%*%V
c(3,1)+P1%*%V
c(3,0)+P2%*%V
V = c(3,1)+P1%*%V
c(3,1)+P1%*%V
c(3,0)+P2%*%V
V = c(3,0)+P2%*%V
c(3,1)+P1%*%V
c(3,0)+P2%*%V
pmax(c(3,1)+P1%*%V, c(3,0)+P2%*%V)
c(pmax(c(3,1)+P1%*%V, c(3,0)+P2%*%V))
for(n in 1:100){
V = c(pmax(c(3,1)+P1%*%V, c(3,0)+P2%*%V))
}
a = which.max(c(c(3,1)[2] + P1[2,]%*%V, c(3,0)[2]+P2[2,]%*%V))
V-min(V)
if (a==1):
if (a==1){
c(3,1)+P1%*%V-V}
else{
if (a==1){
c(3,1)+P1%*%V-V
else
if (a==1){
c(3,1)+P1%*%V-V
else
if (a==1){
c(3,1)+P1%*%V-V}else{
c(3,0)+P2%*%V-V}
c(3,1)[2]
P1[2,]
P2[2,]
# Objective data
objective <- c(6403675.70, 6386215.10, 5906155.91, 4581748.10, 4547870.42, 3569065.95, 3549481.79, 3548735.18, 2746600.25)
data <- data.frame(c(charging_probability, objective))
# Charging probability data
charging_probability <- c(0.74, 0.72, 0.64, 0.55, 0.52, 0.46, 0.41, 0.38, 0.34)
# Objective data
objective <- c(6403675.70, 6386215.10, 5906155.91, 4581748.10, 4547870.42, 3569065.95, 3549481.79, 3548735.18, 2746600.25)
data <- data.frame(c(charging_probability, objective))
View(data)
data <- data.frame(charging_probability, objective)
View(data)
View(data)
model <- lm(objective ~ charging_probability, data = data)
# Print the summary of the linear regression model
summary(model)
model$coefficients
plot(data$charging_probability, data$objective)
# Charging probability data
charging_probability <- c(0.74, 0.72, 0.64, 0.55, 0.52, 0.46, 0.41, 0.38, 0.34) * 10
# Objective data
objective <- c(6403675.70, 6386215.10, 5906155.91, 4581748.10, 4547870.42, 3569065.95, 3549481.79, 3548735.18, 2746600.25)
data <- data.frame(charging_probability, objective)
model <- lm(objective ~ charging_probability, data = data)
# Print the summary of the linear regression model
summary(model)
model$coefficients
# Charging probability data
charging_probability <- c(0.74, 0.72, 0.64, 0.55, 0.52, 0.46, 0.41, 0.38, 0.34) * 10
# Range data
range <- c(65.88, 68.09, 75.89, 84.13, 87.75, 93.7, 98.3, 102.3, 106.5)
# Objective data
objective <- c(6403675.70, 6386215.10, 5906155.91, 4581748.10, 4547870.42, 3569065.95, 3549481.79, 3548735.18, 2746600.25)
data <- data.frame(charging_probability, range, objective)
model <- lm(objective ~ range, data = data)
# Print the summary of the linear regression model
summary(model)
model$coefficients
setwd("~/Documents/MOPTA/mopta/mopta/results")
##########################################################
data <- read.csv('hpTuning.csv')
View(data)
##########################################################
data <- read.csv('hpTuning.csv', sep='\t')
View(data)
View(data)
##########################################################
data <- read.csv('hpTuning.csv', sep='\t', header = False)
##########################################################
data <- read.csv('hpTuning.csv', sep='\t', header = None)
##########################################################
data <- read.csv('hpTuning.csv', sep='\t', header = NA)
##########################################################
data <- read.csv('hpTuning.csv', sep='\t', header = 0)
View(data)
# Fit a linear regression model to determine the influence of w on g3
model_w <- lm(V6 ~ V3, data = data)
# Fit a linear regression model to determine the influence of c1 on g3
model_c1 <- lm(V6 ~ V1, data = data)
# Fit a linear regression model to determine the influence of c2 on g3
model_c2 <- lm(V6 ~ V2, data = data)
# Print the summary of the models
summary(model_w)
summary(model_c1)
summary(model_c2)
# Print the summary of the models
summary(model_w)
summary(model_c1)
summary(model_c2)
summary(model_c1)
summary(model_c2)
# Print the summary of the models
summary(model_w)
summary(model_c1)
